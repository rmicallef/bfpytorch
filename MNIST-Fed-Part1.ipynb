{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Machine Learning in PyTorch\n",
    "\n",
    "This notebook is a demonstration of _federated learning_, an approach to training a machine learning model by combining the results of training local models on multiple devices.\n",
    "\n",
    "This notebook is part of a series intended to start from the basics to demonstrate federated learning in `pytorch`, then move on to test some of the bounds of what federated learning can (and can't) do.\n",
    "\n",
    "## The Task: Digit Recognition\n",
    "\n",
    "For this demonstration, our task is a classic one: the MNIST digit recognition task. Specifically, we need to determine which number is written in an image of a handwritten digit like this one: \n",
    "\n",
    "![Handwritten digit \"6\"](images/example-6.png)\n",
    "\n",
    "To carry out this task we need a machine learning model to \"look\" at an image and classify it into a number. We also need and image data to train the model. We're going to use a simple convolutional neural network model. For the training images, we're using the well-known MNIST digits dataset.\n",
    "\n",
    "## The Approach: Federation\n",
    "\n",
    "The federated approach uses several _worker_ devices to train their own local models with independent data. That is, the workers each train their own local model with a subset of the full dataset.\n",
    "\n",
    "Each worker trains its local model for a bit, then sends its incrementally trained local model back to a _manager_. The manager combines the local models from all of the workers into a master model, then sends the new master model back to the workers. This unit of processing is called a _round_. \n",
    "\n",
    "Then another round begins: the workers each train their local copy of the master model (creating another set of individualized local models), send those updated local models back to the manager. Once again the manager combines these individualized local models into a new master model, and pushes the new master model down to the workers. This cycle of rounds repeats as long as is necessary and useful.\n",
    "\n",
    "## Our Federation Library\n",
    "\n",
    "The `federated` library we're using here is a *simulation* of a federated network. It's not *true* federated learning because it uses one machine to simulate several indepedent machines. As a result of being on one machine, it does not have to deal with network communication or the timing problems inherent to real parallelism. But in our toy example the \"local\" models are independent in that they do not share data nor local models (except to the extent their models are combined) and that is sufficient to demonstrate the principles underlying federated learning.\n",
    "\n",
    "Our library includes a `FederatedManager` class and a `FederatedWorker` class. A `FederatedManager` contains a master prediction model and creates several independent `FederatedWorker` instances. Each `FederatedWorker` has its own local model and a distinct subset of the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "\n",
    "\n",
    "class FederatedManager:\n",
    "\n",
    "    def __init__(self, \n",
    "                 name,\n",
    "                 dataloaders, \n",
    "                 test,\n",
    "                 make_model,\n",
    "                 loss_fn=nn.CrossEntropyLoss(), \n",
    "                 n_epochs=1, \n",
    "                 lr=1e-2, \n",
    "                 verbose=False, \n",
    "                 *args, **kwargs):\n",
    "        \n",
    "        self.dataloaders = dataloaders\n",
    "        self.n_workers = len(dataloaders)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.name = name\n",
    "        self.history = {\"test_loss\": [], \"test_acc\": []}\n",
    "        self.make_model = make_model\n",
    "        self.model = self.make_model()\n",
    "        self.model.train(False)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.Xtest, self.ytest = consume_dataset(test)\n",
    "        self.workers = []\n",
    "        for i, dl in enumerate(dataloaders):\n",
    "            self.workers.append(FederatedWorker(i, \n",
    "                                                self, \n",
    "                                                dl, \n",
    "                                                loss_fn,\n",
    "                                                n_epochs=n_epochs, \n",
    "                                                lr=lr, \n",
    "                                                verbose=verbose,\n",
    "                                                *args,\n",
    "                                                **kwargs))\n",
    "        self.worker_loss_histories = [[] for _ in self.workers]\n",
    "\n",
    "    def round(self):\n",
    "        \"\"\"\n",
    "        Do a round of federated learning:\n",
    "         - instruct each worker to train and return its model\n",
    "         - replace the server model the weighted average of the worker models\n",
    "         - replace the worker models with the server model\n",
    "        Workers with `participant=False` train but are not included in the\n",
    "        weighted average and do not receive a copy of the server model.\n",
    "        \"\"\"\n",
    "        updates = [w.train() for w in self.workers]\n",
    "        self.fedavg(\n",
    "            [u for u, w in zip(updates, self.workers) if w.participant]\n",
    "        )\n",
    "        self.push_model(w for w in self.workers if w.participant)\n",
    "        self.record_loss()\n",
    "\n",
    "    def fedavg(self, updates):\n",
    "        \"\"\"\n",
    "        Replace the manager model with the weighted average of the node models.\n",
    "        \"\"\"\n",
    "        N = sum(u[\"n_samples\"] for u in updates)\n",
    "        for key, value in self.model.state_dict().items():\n",
    "            weight_sum = (\n",
    "                u[\"state_dict\"][key] * u[\"n_samples\"] for u in updates\n",
    "            )\n",
    "            value[:] = sum(weight_sum) / N\n",
    "\n",
    "    def push_model(self, workers):\n",
    "        \"\"\"\n",
    "        Push manager model to a list of workers.\n",
    "        \"\"\"\n",
    "        for worker in workers:\n",
    "            worker.model = self.copy_model()\n",
    "\n",
    "    def copy_model(self):\n",
    "        \"\"\"\n",
    "        Return a copy of the current manager model.\n",
    "        \"\"\"\n",
    "        model_copy = self.make_model()\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "        return model_copy\n",
    "\n",
    "    def evaluate_model(self, model=None):\n",
    "        \"\"\"\n",
    "        Compute the loss and accuracy of model on test set.\n",
    "        \"\"\"\n",
    "        model = model or self.model\n",
    "        was_training = model.training\n",
    "        model.train(False)\n",
    "        with torch.no_grad():\n",
    "            output = model(self.Xtest)\n",
    "            loss = self.loss_fn(output, self.ytest).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct = pred.eq(self.ytest.view_as(pred)).sum().item()\n",
    "        model.train(was_training)\n",
    "        return loss, 100. * correct / len(self.ytest)\n",
    "\n",
    "    def record_loss(self):\n",
    "        \"\"\"\n",
    "        Record loss of manager model and all worker models on test set.\n",
    "        \"\"\"\n",
    "        loss_accuracy = self.evaluate_model()\n",
    "        self.history[\"test_loss\"].append(loss_accuracy[0])\n",
    "        self.history[\"test_acc\"].append(loss_accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each `round` of federated learning, each `FederatedWorker` trains its local model using only the data that that worker recieves, and then sends its trained local model to the `FederatedManager`. The `FederatedManager` combines the models into an updated master model and pushes that new master model down to each `FederatedWorker`. There are a number of algorithms available for combining local models. Our library uses [federated averaging](https://arxiv.org/abs/1602.05629), a very simple element-by-element average of the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedWorker:\n",
    "\n",
    "    def __init__(\n",
    "        self, name, manager, dataloader, loss_fn, n_epochs=1, lr=1e-2,\n",
    "        momentum=0.5, participant=True, verbose=False\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.manager = manager\n",
    "        self.dataloader = dataloader\n",
    "        self.n_epochs = n_epochs\n",
    "        self.loss_fn = loss_fn\n",
    "        self.participant = participant\n",
    "        self.model = manager.copy_model()\n",
    "        self.n_samples = len(self.dataloader.dataset)\n",
    "        self.history = {\"train_loss\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train for n_epochs, then return the state dictionary of the model and\n",
    "        the amount of training data used.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr,\n",
    "                                    momentum=self.momentum)\n",
    "        self.model.train(True)\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for i, (x, y) in enumerate(self.dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                ypred = self.model(x)\n",
    "                train_loss = self.loss_fn(ypred, y)\n",
    "                train_loss.backward()\n",
    "                optimizer.step()\n",
    "                self.history[\"train_loss\"].append(train_loss.item())\n",
    "\n",
    "        loss_accuracy = self.manager.evaluate_model(self.model)\n",
    "        self.history[\"test_loss\"].append(loss_accuracy[0])\n",
    "        self.history[\"test_acc\"].append(loss_accuracy[1])\n",
    "        \n",
    "        if(self.verbose):\n",
    "            print(\n",
    "                '\\twrkr {}\\t\\tloss: {:.4f}\\tacc: {:.2%}'.format(\n",
    "                    self.name,\n",
    "                    self.history[\"test_loss\"][-1],\n",
    "                    self.history[\"test_acc\"][-1] / 100,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"n_samples\": self.n_samples\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Now we define the model. We're using the [sample model from the `pytorch` package](https://github.com/pytorch/examples/blob/master/mnist/main.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#import torch.optim\n",
    "\n",
    "'''   \n",
    "\n",
    "# THIS IS THE OFFICIAL PYTORCH EXAMPLE, BUT IT'S SLOWER THAN THE OTHER MODEL, SO FOR NOW, IT'S FAST BEFORE TIDY\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "'''\n",
    "\n",
    "# Quick and gets the job done well enough for testing\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to split the datasets up for testing federation\n",
    "\n",
    "def consume_dataset(dataset):\n",
    "    data = list(zip(*dataset))\n",
    "    X = torch.stack(data[0])\n",
    "    y = torch.tensor(data[1])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data - Standard MNIST Datasets\n",
    "Here we get plain vanilla MNIST data, a training set and a test set, from the `pytorch` package, again using the same transforms as their example.\n",
    "\n",
    "If you don't already have the MNIST data on your machine, the setting `download=True` in the call to `torchvision.datasets.MNIST()` will fetch it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# create standard datasets using all of the MNIST data\n",
    "data_path = './MNIST-data/raw'\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dset = dsets.MNIST(root=data_path, download=True, train=True, transform=trans)\n",
    "test_dset = dsets.MNIST(root=data_path, download=True, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our standard MNIST `Dataset`s: `train_dset` contains 60,000 examples, and `test_dset` contains 10,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset size: \", len(train_dset))\n",
    "print(\"Test dataset size : \", len(test_dset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch uses `DataLoader`s for training. A `DataLoader` contains a `Dataset` and training parameters like `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "\n",
    "# create standard dataloaders using all of the MNIST data\n",
    "train_dl = DataLoader(train_dset, batch_size=train_batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_dset, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the default data. The out-of-the-box MNIST dataset has roughly equal numbers of samples for each digit, i.e., about as many samples of `4`s as `6`s (though the set is a *little* heavy on `1`s and a *little* light on `5`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "#import federated as fed\n",
    "#import federated_mnist as fm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_title('Standard MNIST Digit Counts')\n",
    "ax.set_xlabel('Digit')\n",
    "ax.set_ylabel('Digit Samples')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.hist([train_dl.dataset.targets.tolist(), test_dl.dataset.targets.tolist()*6], \n",
    "        label=['Train', 'Test (scaled)'],\n",
    "        bins=list(range(11)), \n",
    "        histtype='bar',\n",
    "        align='left',\n",
    "        rwidth=0.8,\n",
    "       )\n",
    "ax.legend();\n",
    "'''\n",
    "\n",
    "def plot_digit_histogram(dls, title=''):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.set_ylabel('Digit Samples')\n",
    "    ax.set_xlabel('Digit')\n",
    "    \n",
    "    ys = []\n",
    "    for dset in tqdm([dl.dataset for dl in dls], desc='Tabulating datasets'):\n",
    "        _, y = consume_dataset(dset)\n",
    "        ys.append(sorted(Counter(y)))\n",
    "        \n",
    "    H = ax.hist(ys, bins=range(11), histtype='bar', align='left', rwidth=0.8)\n",
    "\n",
    "\n",
    "plot_digit_histogram([train_dl, test_dl], 'MNIST Complete Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is reasonably well distributed, and the plots above confirm it. The test set counts are smaller by a factor of six, which reflects the relative sizes of the train and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: The Non-Federated Approach\n",
    "\n",
    "To show baselines of non-federated learning, we'll train a model the traditional way, i.e., using just one worker. (Technically we'll using our federated code, but with one worker that's equivalent to non-federated learning.)\n",
    "\n",
    "Let's get some basic housekeeping out of the way. These functions are convenient wrappers to simpulate systems to a certain endpoint (target accuracy or number of rounds) and show their output. (For every example in this post we use the full 10,000 sample MNIST test set to evaluate our models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default training targets\n",
    "default_n_rounds = 10\n",
    "default_target_acc = 95.0\n",
    "\n",
    "def run_mgr(mgr, n_rounds, target_acc=None):\n",
    "    \n",
    "    target_met = False;\n",
    "    \n",
    "    if (target_acc):\n",
    "        print('{} manager training with {} worker(s) for up to {} rounds or {:.2%} accuracy.'.format(\n",
    "                mgr.name, mgr.n_workers, n_rounds, target_acc / 100,))\n",
    "    else:\n",
    "        print('{} manager training with {} worker(s) for {} rounds.'.format(\n",
    "                mgr.name, mgr.n_workers, n_rounds,))\n",
    "\n",
    "    for i in trange(n_rounds, desc='Rounds'):\n",
    "        if(mgr.verbose):\n",
    "            print('Round', i)\n",
    "        mgr.round()\n",
    "        if(mgr.verbose):\n",
    "            print('\\tcombined\\tloss: {:.4f}\\tacc: {:.2%}\\n'.format(\n",
    "                mgr.history['test_loss'][-1], mgr.history['test_acc'][-1] / 100,))\n",
    "            \n",
    "        if(target_acc and (mgr.history['test_acc'][-1] >= target_acc)):\n",
    "            target_met = True\n",
    "            break;\n",
    "\n",
    "    if(target_met):\n",
    "        print('{} manager stopped: met accuracy target of {:.2%} after {} rounds. (Test accuracy {:.2%} and loss {:.4f}.)'.format(\n",
    "                mgr.name, target_acc / 100, len(mgr.history['test_acc']), mgr.history['test_acc'][-1] / 100, mgr.history['test_loss'][-1],))\n",
    "    else:\n",
    "        print('{} manager trained {} rounds. (Test accuracy {:.2%} and loss {:.4f}.)'.format(\n",
    "                mgr.name, len(mgr.history['test_acc']), mgr.history['test_acc'][-1] / 100, mgr.history['test_loss'][-1],))\n",
    "\n",
    "def plot_mgr(mgrs, plot_workers=False):\n",
    "    \n",
    "    if not isinstance(mgrs, list):\n",
    "        mgrs = [mgrs]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for m in mgrs:\n",
    "        ax.plot(m.history['test_loss'], label=m.name)\n",
    "        if(plot_workers):\n",
    "            for w in m.workers:\n",
    "                ax.plot(w.history['test_loss'], label=(m.name, 'Worker ' + str(w.name)))\n",
    "    ax.set_xlabel(\"Round\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend();\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    for m in mgrs:\n",
    "        ax.plot(m.history['test_acc'], label=m.name)\n",
    "        if(plot_workers):\n",
    "            for w in m.workers:\n",
    "                ax.plot(w.history['test_acc'], label=(m.name, 'worker ' + str(w.name)))\n",
    "    ax.set_xlabel(\"Round\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend();\n",
    "\n",
    "'''\n",
    "def eval_new_mgr(name, train_dl, test_dset, p=0.0, n_rounds=50, target_acc=None):\n",
    "    \n",
    "    dls = make_fed_dloaders(train_dl, p=p)\n",
    "    mgr = make_mgr(name, dls, test_dset)\n",
    "    run_mgr(mgr, n_rounds, target_acc=target_acc)\n",
    "    plot_mgr(mgr)\n",
    "    return mgr\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perfect World: All the Data in One Place\n",
    "\n",
    "Now we can simulate an ideal non-federated situation where all of our data is in one place to train a master model. To do this we'll train a model using all of the data in the MNIST training set, 60,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size: \", len(train_dl.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we're simulating *non-federated* learning by using a \"federated\" system with only one worker - these are functionally equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfed_alldata_mgr = FederatedManager('Non-Federated Full Dataset', [train_dl], test_dset, Net)\n",
    "\n",
    "run_mgr(nonfed_alldata_mgr, default_n_rounds, target_acc=default_target_acc)\n",
    "\n",
    "plot_mgr(nonfed_alldata_mgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's our baseline ideal-world situation, a non-federated model trained with all of the data in front of it. Let's check out less-ideal situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Federated Learning With Limited Data\n",
    "\n",
    "In the real world, independent devices won't have all the data in front of them, and there will be bias in the data they see. E.g., a smartphone won't have access to other user's data to train a predictive text model. So let's see what happens when we simulate just one device with less data and some skew in that limited data.\n",
    "\n",
    "The code below slices up the standard MNIST dataset and samples it without replacement to make ten subsets, then makes a dataloader for each of the ten subsets. The parameter `p` reflects the amount of skew in the dataset. There's also an eval function for doing a whole simulation on one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "def make_fed_dloaders(dset, p=None, batch_size=64, shuffle=True):\n",
    "    \n",
    "    return [DataLoader(dset, batch_size, shuffle) for dset in make_fed_dsets(dset, p=p)]\n",
    "    \n",
    "def make_fed_dsets(dset, p=None):\n",
    "    _, y = consume_dataset(dset)\n",
    "    classes = set(y.numpy())\n",
    "    n_classes = len(classes)\n",
    "    idx_dset = index_to_dataset(y, p=p)\n",
    "    dset_idx = [np.where(idx_dset == di)[0] for di in range(n_classes)]\n",
    "    return [Subset(dset, di) for di in dset_idx]\n",
    "\n",
    "def index_to_dataset(y, p=None):\n",
    "    classes = set(y.numpy())\n",
    "    n_classes = len(classes)\n",
    "    p = p or 1/n_classes\n",
    "    pnot = (1-p)/(n_classes-1)\n",
    "    ps = np.full((n_classes, n_classes), pnot)\n",
    "    np.fill_diagonal(ps, p)\n",
    "    return np.array([np.random.choice(10, p=ps[yi]) for yi in y])\n",
    "\n",
    "def eval_new_mgr(name, train_dset, test_dset, p=0.0, n_rounds=50, target_acc=None, net=Net):\n",
    "    dls = make_fed_dloaders(train_dset, p=p)\n",
    "    mgr = FederatedManager(name, dls, test_dset, net)\n",
    "    run_mgr(mgr, n_rounds, target_acc)\n",
    "    plot_mgr(mgr)\n",
    "    return mgr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a sample dataset with about 6,000 training samples, one tenth of the total MNIST training set. We'll arbitrarily choose the digit `4` for our overweighted digit, and a relatively mild skew value of `p=0.25`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of DataLoaders and use the one that is skewed toward 4s\n",
    "\n",
    "nonfed_four_dl = make_fed_dloaders(train_dset, p=0.25)[4]\n",
    "\n",
    "plot_digit_histogram([nonfed_four_dl], 'MNIST Four-Heavy Dataset')\n",
    "\n",
    "print(\"Dataset size: \", len(nonfed_four_dl.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our dataset looks like. About a tenth of the samples and heavy on the 4s.\n",
    "\n",
    "Now let's see what happens when we train with this subset. As before, we won't use any federation here, just less data and a little bias in the data we do have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfed_four_mgr = FederatedManager('Non-Federated Fours Biased 0.25', [nonfed_four_dl], test_dset, Net)\n",
    "\n",
    "run_mgr(nonfed_four_mgr, default_n_rounds, default_target_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this result to our ideal-world simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr([nonfed_alldata_mgr, nonfed_four_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that the limited-data model converges more slowly than the full data model. (I say \"should\" because these are randomly sampled datasets we're building - there's always a very small chance we'll get an unlucky draw.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how much of the problem here is bias in the data, and how much is the limited number of samples? Let's try a non-federated simulation with the same number of samples, but with no bias in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nonfed_subset_mgr = eval_new_mgr('Non-Federated Subset Unbiased', make_fed_dloaders(train_dset, p=0.0)[4], test_dset)\n",
    "nonfed_subset_mgr = FederatedManager('Non-Federated Subset Unbiased', [make_fed_dloaders(train_dset, p=0.0)[4]], test_dset, Net)\n",
    "run_mgr(nonfed_subset_mgr, default_n_rounds, default_target_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare our three Non-federated simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr([nonfed_alldata_mgr, nonfed_four_mgr, nonfed_subset_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it really is mostly the size of the dataset, not the bias, at least at `p=0.25`, that hurts performance. Federation gives us access to the signal in more data without moving the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Federated Approach\n",
    "\n",
    "Now that we have a baseline using the traditional, i.e., non-federated, approach, let's see what federation gets us (and doesn't). We'll do the same MNIST task using federation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Deck Into Piles\n",
    "\n",
    "To illustrate federated learning we use several workers, each representing a remote device. We'll use ten workers here, and for now, we'll assume each of our ten workers has roughly equal types and quantities of data to the other nine (to do this we use `p=0` to reflect roughly even distribution of digit samples for any one worker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dsets(dsets):\n",
    "    for dset in dsets:\n",
    "        _, y = consume_dataset(dset)\n",
    "        print(Counter(y.numpy()))\n",
    "\n",
    "fed_equal_dls = make_fed_dloaders(train_dset)\n",
    "\n",
    "check_dsets([dl.dataset for dl in fed_equal_dls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit_histogram(fed_equal_dls, 'Digit Counts per Dataloader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that each of our ten workers has about the same number of each digit, i.e., the digit `5` is spread pretty evenly across the workers, and is probably lower in number for any given worker than the digit `1`, `1` being the most frequent digit in the MNIST set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_equal_mgr = FederatedManager('Federated Unbiased', fed_equal_dls, test_dset, Net)\n",
    "\n",
    "run_mgr(fed_equal_mgr, default_n_rounds, default_target_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr([nonfed_alldata_mgr, fed_equal_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The federated model converges, though a little slower than the non-federated version. But in the federated model, we don't have to move any data to a central system.\n",
    "\n",
    "But this comparison isn't quite apples to apples. How does our fully federated model compare to a single user with limited data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr([nonfed_subset_mgr, fed_equal_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The federated manager, which uses all the data, but doesn't have all the data in one place, outperforms the non-federated model. In other words, performance improves with the additional signal made available by federation, and without the cost and risk of data transfer from the worker devices.\n",
    "\n",
    "Of course, this isn't a guarantee in every case, but it shows the mechanics and potential of federation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federation with Varying Example Distributions Per Worker\n",
    "\n",
    "Now we've seen federation work with a dataset scattered across a number of workers. And we saw that its performance isn't substantially different from the non-federated approach, while decentralizing the work of training and limiting the amount of data transfered between the main manager and the workers.\n",
    "\n",
    "But what if our workers don't have access to equal amounts of data? Let's explore that.\n",
    "\n",
    "## Spliting the Decks into Uneven Piles\n",
    "\n",
    "To test this, we need workers to have access to varying numbers of examples across training classes. So let's make a set of dataloaders that will give each worker more examples of a given digit than the other workers.\n",
    "\n",
    "The parameter `p` sets the degree of bias toward the overweighted digit in each dataloader. Specifically, `p` is the odds of an example from a specific class being selected for a worker from the base dataset. We'll use `0.15` for `p` for now.\n",
    "\n",
    "This results in a set of stacked dataloaders. The number of dataloaders is equal to the number of classes. Each dataloader is overweighted to one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_weighted_dls = make_fed_dloaders(train_dset, p=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataloaders still give each worker roughly the same *total number* of examples. It's the number of examples *per class* that varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_example_counts = [len(dl.dataset) for dl in fed_weighted_dls]\n",
    "\n",
    "print(worker_example_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And each of the 60,000 examples in the MNIST training dataset is allocated (without replacement) to a worker, so the total number of examples across workers is 60,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(worker_example_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the digit examples are allocated within each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit_histogram(fed_weighted_dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that worker 4, shown in purple, has more examples of the digit `4` than other workers. Likewise, worker 6, shown in pink, has more examples of the digit `6` than other workers.\n",
    "\n",
    "Here are the actual counts from the dataloaders for workers 4 and 6. You can see that the most frequent examples in those dataloaders are `4` and `6`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_dsets([fed_weighted_dls[4].dataset])\n",
    "\n",
    "check_dsets([fed_weighted_dls[6].dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you're curious, here are all of the worker counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in fed_weighted_dls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the federated approach performs with variation in the worker datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_weighted_mgr = FederatedManager('Fed Bias', fed_weighted_dls, test_dset, Net)\n",
    "run_mgr(fed_weighted_mgr, default_n_rounds, default_target_acc)\n",
    "plot_mgr(fed_weighted_mgr)\n",
    "\n",
    "#fed_060_mgr = eval_new_mgr('Fed Bias 0.60', train_dset, test_dset, p=0.60, num_rounds=10, target_acc=None)\n",
    "\n",
    "#plot_mgr(fed_060_mgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biased federated model converges just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr([nonfed_alldata_mgr, fed_equal_mgr, fed_weighted_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it converges a little slower than the unbiased federated model.\n",
    "\n",
    "Let's take a closer look at how performance changes with increasingly biased data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs = [eval_new_mgr(('Fed Bias '+ str(x)), train_dset, test_dset, p=x, n_rounds=10, target_acc=90.0) for x in tqdm([0.0, 0.1, 0.6, 0.9, 0.95, 0.98, 0.999, 1.0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr(mgrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The federated models converge even with very heavily biased datasets per worker. Unsurprisingly, it takes longer to hit a given target accuracy. But even when the datasets are completely biased, that is, each worker sees examples from *exactly one class*, federation works.\n",
    "\n",
    "We expect that a sole learner with access to samples of only one class will perform poorly on a ten-class task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfed_allfours_dl = make_fed_dloaders(train_dset, p=1.0)[4]\n",
    "\n",
    "nonfed_allfours_mgr = FederatedManager('Non-Federated Fours Biased 1.0', [nonfed_allfours_dl], test_dset, Net)\n",
    "\n",
    "run_mgr(nonfed_allfours_mgr, default_n_rounds, default_target_acc)\n",
    "\n",
    "plot_mgr(nonfed_allfours_mgr)\n",
    "\n",
    "#nonfed_allfours_mgr = eval_new_mgr('Non-Federated Fours Biased 1.0', train_dset, test_dset, p=1.0, n_rounds=default_n_rounds, target_acc=default_target_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprise - it doesn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs.append(nonfed_allfours_mgr)\n",
    "\n",
    "plot_mgr(mgrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases of heavily skewed datasets, federation enables the master model to incorporate signal from sources that operate only as sensors for outlier data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "TODO: WRAP UP THE BLOG POST HERE. EVERYTHING BELOW IS PART TWO OR THREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVERYTHING BELOW HERE IS SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs2 = [eval_new_mgr(('Fed Bias '+ str(x)), train_dset, test_dset, p=x, n_rounds=100, target_acc=None) for x in tqdm([0.0, 0.4, 0.6, 0.8, 0.9, 0.95, 0.98, 0.99, 1.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr(mgrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs3 = [eval_new_mgr(('Fed Bias '+ str(x)), train_dset, test_dset, p=x, n_rounds=300, target_acc=None) for x in tqdm([0.0, 0.4, 0.6, 0.8, 0.9, 0.95, 0.98, 0.99, 1.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr(mgrs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrs4 = [eval_new_mgr(('Fed Bias '+ str(x)), train_dset, test_dset, p=x, n_rounds=300, target_acc=None) for x in tqdm([0.9, 0.95, 0.98, 0.99, 0.992, 0.995, 0.997, 0.999, 1.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mgr(mgrs4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_bias_dls = make_fed_dloaders(train_dset, p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in full_bias_dls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.995)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.995)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.995)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.996)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.997)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.998)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.9992)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.9992)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in.make_fed_dloaders(train_dset, p=0.9992)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.9995)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.9997)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.9998)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=0.9999)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=1.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=1.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=1.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dsets([dl.dataset for dl in make_fed_dloaders(train_dset, p=1.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_test_dump = json.dumps(fed_weighted_mgr.__dict__)\n",
    "print(json_test_dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO? Persist parameters, histories, results for every run. JSON?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a skewing function that allows skewing multiple variables.\n",
    "\n",
    "How does performance change if we have high-entropy class corellation? Put 1s and 7s together and 3s and 8s. Does this help more than 1s and 8s and 7s and 3s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
