{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Machine Learning\n",
    "\n",
    "This notebook is a demonstration of _federated learning_, an approach to training a machine learning model by combining the results of training local models on multiple devices. This notebook is part of a series intended to demonstrate federated learning, then move on to test some of the bounds of what federated learning can (and can't) do. We'll be using `pytorch` as our machine learning tool.\n",
    "\n",
    "## The Task: Digit Recognition\n",
    "\n",
    "For this demonstration, our task is a classic one: the MNIST digit recognition task. Specifically, we need to determine which number is written in an image of a handwritten digit like this one: \n",
    "\n",
    "![Handwritten digit \"6\"](images/example-6.png)\n",
    "\n",
    "To carry out this task we need a machine learning model to \"look\" at an image and classify it into a number. We also need and image data to train the model. We're going to use a simple convolutional neural network model from the [`pytorch` example code](https://github.com/pytorch/examples/blob/master/mnist/main.py) (to keep it simple and to keep the focus on federation, we'll be using whatever we can from the standard `pytorch` examples).\n",
    "\n",
    "## The Approach to Demonstrate: Federation\n",
    "\n",
    "The federated approach uses several _worker_ devices to train their own local models with independent data. That is, the workers each train their own local model with a subset of the full dataset.\n",
    "\n",
    "Each worker trains its local model for a bit, then sends its incrementally trained local model back to a _manager_. The manager combines the local models from all of the workers into a common model, then sends the new common model back to the workers. This unit of processing is called a _round_. \n",
    "\n",
    "Then another round begins: the workers each train their local copy of the common model (creating another set of individualized local models), send those updated local models back to the manager. Once again the manager combines these individualized local models into a new common model, and pushes the new common model down to the workers. This cycle of rounds repeats as long as is necessary and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Elements\n",
    "\n",
    "Let's first set a baseline so we have a point of reference for our federated approach. To keep the comparisons useful, we'll use the same overall dataset and neural network in the baseline case as we do in the federated cases. Let's set those up here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data - Standard MNIST Datasets\n",
    "Here we get the MNIST data, a training set and a test set, from the `pytorch` package, again using the same transforms and normalization parameters as the ['pytorch' example code](https://github.com/pytorch/examples/blob/master/mnist/main.py). The standard MNIST training and test datasets, contain 60,000 and 10,000 examples, respectively.\n",
    "\n",
    "Pytorch uses `DataLoader`s for training. We'll be working with a lot of them here. A `DataLoader` contains a `Dataset` and training parameters like `batch_size`. We'll make a couple of standard `DataLoader`s with the parameters used in the [`pytorch` example code](https://github.com/pytorch/examples/blob/master/mnist/main.py). These contain the 60,000-example training `Dataset` and the 10,000-example test `Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 60000\n",
      "Testing dataset size: 10000\n"
     ]
    }
   ],
   "source": [
    "from mnist_utils import get_MNIST_dataloaders\n",
    "\n",
    "default_training_dataloader, default_testing_dataloader = get_MNIST_dataloaders()\n",
    "\n",
    "default_training_dataset = default_training_dataloader.dataset\n",
    "default_testing_dataset = default_testing_dataloader.dataset\n",
    "\n",
    "print('Training dataset size:', len(default_training_dataset))\n",
    "print('Testing dataset size:', len(default_testing_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm being super explicit about the digit counts and distributions here, because we're going to be doing some fancy footwork with them later on. So we need to know what we're starting with. So let's check out the default MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83d9bebfa034741b352b14cb63202e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Tabulating datasets', max=2.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mnist_utils import plot_digit_histogram\n",
    "\n",
    "plot_digit_histogram([default_training_dataloader, default_testing_dataloader], title='MNIST Complete Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out-of-the-box MNIST dataset has roughly equal numbers of samples for each digit, i.e., about as many samples of `4`s as `6`s (though the set is a *little* heavy on `1`s and a *little* light on `5`s). The test set (orange) counts are smaller than the training set (blue) counts by a factor of six, which reflects the relative sizes of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: The Non-Federated Approach\n",
    "To show baselines of non-federated learning, we'll train a model the traditional way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perfect World: All the Data in One Place\n",
    "\n",
    "We're testing an ideal non-federated situation: all of our data is in one place to train a single model. To do this we'll train that single model with all of the data in the MNIST training set, 60,000 examples. We'll set a default number of rounds to train. In the non-federated case, a round is equivalent to an epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 60000\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", len(default_training_dataloader.dataset))\n",
    "\n",
    "# default training rounds\n",
    "default_n_rounds = 50\n",
    "\n",
    "# target for early stopping (we'll use this later to compare approaches)\n",
    "default_target_accuracy = 99.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model in all of these examples is the convolutional neural network used in the [`pytorch` sample code](https://github.com/pytorch/examples/blob/master/mnist/main.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_utils import DefaultNet\n",
    "\n",
    "model = DefaultNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train without federation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3521a81bbbe3423e8dbe86c4738a177f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0 loss: 0.0665 accuracy: 97.99%\n",
      "Round: 1 loss: 0.0429 accuracy: 98.69%\n",
      "Round: 2 loss: 0.0493 accuracy: 98.75%\n",
      "Round: 3 loss: 0.0360 accuracy: 98.84%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0168099a64ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtraining_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mtraining_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ryan\\work\\bfpytorch\\venv\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \"\"\"\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ryan\\work\\bfpytorch\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "from mnist_utils import split_dataset\n",
    "\n",
    "n_epochs = default_n_rounds\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.) # this is used in the example code\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "dataloader = default_training_dataloader\n",
    "test_samples, test_labels = split_dataset(default_testing_dataset)\n",
    "\n",
    "history = {'test_loss': [], 'test_accuracy': []}\n",
    "\n",
    "#model = model.to(device)\n",
    "\n",
    "for epoch in trange(n_epochs):\n",
    "    \n",
    "    #train\n",
    "    model.train(True)\n",
    "    for i, (training_samples, training_labels) in enumerate(dataloader):\n",
    "        \n",
    "#        training_samples.to(device)\n",
    "#        training_labels.to(device)\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        training_predictions = model(training_samples)\n",
    "        training_loss = loss_function(training_predictions, training_labels)\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    #test\n",
    "    model.train(False)\n",
    "    with torch.no_grad():\n",
    "        output = model(test_samples)\n",
    "        \n",
    "#        output = output.to('cpu')\n",
    "        \n",
    "        test_loss = loss_function(output, test_labels).item()\n",
    "        test_predicted_classes = output.argmax(dim=1, keepdim=True)\n",
    "        n_correct = test_predicted_classes.eq(test_labels.view_as(test_predicted_classes)).sum().item()\n",
    "    accuracy = (n_correct / len(test_labels)) * 100.\n",
    "\n",
    "    #record loss and accuracy\n",
    "    history['test_loss'].append(test_loss)\n",
    "    history['test_accuracy'].append(accuracy)\n",
    "    print('Round: {} loss: {:.4f} accuracy: {:.2%}'.format(epoch, test_loss, accuracy / 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history['test_loss'], label='Non-Federated')\n",
    "ax.set_xlabel(\"Round\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend();\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history['test_accuracy'], label='Non-Federated')\n",
    "ax.set_xlabel(\"Round\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's our baseline ideal-world situation, a non-federated model trained with all 60,000 data points in front of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Federated Approach\n",
    "\n",
    "Now that we have a baseline using the traditional, i.e., non-federated, approach, let's see what federation gets us (and doesn't). We'll do the same MNIST task using federation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Federation Code\n",
    "\n",
    "The federation code we're using here is a *simulation* of a federated network. It's not *true* federated learning because it uses one machine to simulate several indepedent machines. As a result of being on one machine, it does not have to deal with network communication or the timing problems inherent to real parallelism. But in our toy example the \"local\" models are independent in that they do not share data nor local models (except to the extent their models are combined) and that is sufficient to demonstrate the principles underlying federated learning.\n",
    "\n",
    "Our library includes a `FederatedManager` class and a `FederatedWorker` class. A `FederatedManager` contains a common prediction model and creates several independent `FederatedWorker` instances. Each `FederatedWorker` has its own local model and a distinct subset of the MNIST data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "This is fine but There's some text I used to explain why this \"not true federated learning\" thing doesn't mean what you're reading is useless. I like \"algorthmically faithful\". It's also a bit tricky to explain what this simulation approach _doesn't_ do (network traffic, race conditions, etc.) until you've explained the algorithm. So I think this section needs a bit of work.\n",
    "\n",
    "Also the heading is not a good indication of its purpose. This is in some ways the most important bit of the article for a reader who is new to FL! It's the bit where we explain what FL is!\n",
    "\n",
    "Can we use some figures from the report to make it easier to follow?\n",
    "\n",
    "Also, backing up a bit: I'm not sure you explain _why_ you want to do FL in the first place at any point in this article (privacy, bandwidth, power, etc.) That needs to be somewhere early, probably right at the top, and then repeated immediately after you explain the algorithm: i.e. the algorithm takes care of the concerns I mentioned at the top.\n",
    "\n",
    "\"Our library\": unless you move it into a py file it's not really a library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each `round` of federated learning, each `FederatedWorker` trains its local model using only the data that that worker recieves, and then sends its trained local model to the `FederatedManager`. The `FederatedManager` combines the models into an updated common model and pushes that new common model down to each `FederatedWorker`. There are a number of algorithms available for combining local models. Our library uses [federated averaging](https://arxiv.org/abs/1602.05629), a very simple element-by-element average of the model's weights.\n",
    "\n",
    "```\n",
    "N = sum(u[\"n_samples\"] for u in updates)\n",
    "        for key, value in self.model.state_dict().items():\n",
    "            weight_sum = (\n",
    "                u[\"state_dict\"][key] * u[\"n_samples\"] for u in updates\n",
    "            )\n",
    "            value[:] = sum(weight_sum) / N\n",
    "```\n",
    "\n",
    "Thats the whole combination function, an item-wise average of the model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Deck Into Piles for Federation\n",
    "\n",
    "To illustrate federated learning we use several workers, each representing a remote device. We'll use ten workers here, and for now, we'll assume each of our ten workers has roughly equal types and quantities of data to the other nine (to do this we use `p=0` to reflect roughly even distribution of digit samples for any one worker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to have ten workers. So we allocate each example in the 60,000 sample MNIST training set to one worker. We do this by making ten torch DataLoaders, one for each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist_utils import make_federated_dataloaders\n",
    "\n",
    "federated_equal_dataloaders = make_federated_dataloaders(default_training_dataset)\n",
    "\n",
    "plot_digit_histogram(fed_equal_dls, 'Digit Counts per DataLoader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the allocation worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each of our ten workers has about the same number of each digit, i.e., the digit `5` is spread pretty evenly across the workers, and is probably lower in number for any given worker than the digit `1`, `1` being the most frequent digit in the MNIST set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Training\n",
    "Now let's train the federated system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from federated import FederatedManager\n",
    "from federated import FederatedWorker\n",
    "from federated import plot_managers\n",
    "\n",
    "fed_equal_mgr = FederatedManager('Federated Unbiased', fed_equal_dls, default_testing_dataset, DefaultNet)\n",
    "\n",
    "fed_equal_mgr.learn(default_n_rounds, default_target_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "nonfed_alldata_mgr = FederatedManager('Non-Federated', [default_training_dataloader], default_testing_dataset, DefaultNet)\n",
    "nonfed_alldata_mgr.history = history\n",
    "\n",
    "plot_managers([nonfed_alldata_mgr, fed_equal_mgr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The federated model converges, though a little slower than the non-federated version. But in the federated model, we don't have to move any data to a central system.\n",
    "\n",
    "So we know that there's a performance cost (slower convergence and possibly lower accuracy) to using federation compared to moving all of the data to a central model. But this comparison isn't quite apples to apples. If moving all of the data isn't possible or desirable, the options are federation or just using the local data for each model. So let's compare those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "I don't think this comparison is necessary for the goals of the post, but thanks for doing it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfed_subset_mgr = FederatedManager('Non-Federated Limited Unbiased', [make_federated_dataloaders(default_training_dataset, p=0.0)[7]], default_testing_dataset, DefaultNet)\n",
    "nonfed_subset_mgr.learn(default_n_rounds, default_target_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_managers([fed_equal_mgr, nonfed_subset_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The federated manager, which uses all the data, but doesn't have all the data in one place, matches or outperforms the non-federated model. In other words, performance improves with the additional signal made available by federation, and without the cost and risk of data transfer from the worker devices.\n",
    "\n",
    "Of course, this isn't a guarantee in every case, but it shows the mechanics and potential of federation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Federated Learning With Limited Data\n",
    "\n",
    "In the real world, independent devices won't have all the data in front of them, and there will be bias in the data they see. E.g., a smartphone won't have access to other user's data to train a predictive text model. So let's see what happens when we simulate just one device with less data and some skew in that limited data.\n",
    "\n",
    "The code below slices up the standard MNIST dataset and samples it without replacement to make ten subsets, then makes a dataloader for each of the ten subsets. The parameter `p` reflects the amount of skew in the dataset. There's also a dataset inspection function and an eval function for doing a whole simulation on one line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment\n",
    "I also don't think this section is necessary. I mean logically it's necessary to prove your point, but it's not necessary to make sense of the article. I would drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a sample dataset with about 6,000 training samples, one tenth of the total MNIST training set. We'll arbitrarily choose the digit `4` for our overweighted digit, and a relatively mild skew value of `p=0.25`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of DataLoaders and use the one that is skewed toward 4s\n",
    "\n",
    "nonfed_four_dl = make_federated_dataloaders(default_training_dataset, p=0.25)[4]\n",
    "\n",
    "plot_digit_histogram([nonfed_four_dl], 'MNIST Four-Heavy Dataset')\n",
    "\n",
    "print(\"Dataset size: \", len(nonfed_four_dl.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what our dataset looks like. About a tenth of the samples and heavy on the 4s.\n",
    "\n",
    "Now let's see what happens when we train with this subset. As before, we won't use any federation here, just less data and a little bias in the data we do have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonfed_four_mgr = FederatedManager('Non-Federated Fours Biased 0.25', [nonfed_four_dl], default_testing_dataset, DefaultNet)\n",
    "nonfed_four_mgr.learn(default_n_rounds, default_target_accuracy)\n",
    "plot_managers([nonfed_alldata_mgr, nonfed_four_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that the limited-data model converges more slowly than the full data model. (I say \"should\" because these are randomly sampled datasets we're building - there's always a very small chance we'll get an unlucky draw.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federation with Varying Example Distributions Per Worker\n",
    "\n",
    "Now we've seen federation work with a dataset scattered across a number of workers. And we saw that its performance isn't substantially worse than the non-federated approach, while decentralizing the work of training and limiting the amount of data transfered between the main manager and the workers.\n",
    "\n",
    "But what if our workers don't have access to equal amounts of data? Let's explore that.\n",
    "\n",
    "## Spliting the Decks into Uneven Piles\n",
    "\n",
    "To test this, we need workers to have access to varying numbers of examples across training classes. So let's make a set of dataloaders that will give each worker more examples of a given digit than the other workers.\n",
    "\n",
    "The parameter `p` sets the degree of bias toward the overweighted digit in each dataloader. Specifically, `p` is the odds of an example from a specific class being selected for a worker from the base dataset. We'll use `0.15` for `p` for now.\n",
    "\n",
    "This results in a set of stacked dataloaders. The number of dataloaders is equal to the number of classes. Each dataloader is overweighted to exactly one digit class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_weighted_dls = make_federated_dataloaders(default_training_dataset, p=0.4)\n",
    "worker_example_counts = [len(dataloader.dataset) for dataloader in fed_weighted_dls]\n",
    "\n",
    "print(worker_example_counts)\n",
    "print(sum(worker_example_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the dataloaders still give each worker roughly the same *total number* of examples. It's the number of examples *per class* that varies. And each of the 60,000 examples in the MNIST training dataset is allocated (without replacement) to a worker, so the total number of examples across workers is 60,000.\n",
    "\n",
    "Let's look at how the digit examples are allocated within each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digit_histogram(fed_weighted_dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that worker 4, shown in purple, has more examples of the digit `4` than other workers. Likewise, worker 6, shown in pink, has more examples of the digit `6` than other workers.\n",
    "\n",
    "Here are the actual counts from the dataloaders for workers 4 and 6. You can see that the most frequent examples in those dataloaders are `4` and `6`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mnist_utils import check_datasets\n",
    "\n",
    "check_datasets([fed_weighted_dls[4].dataset])\n",
    "\n",
    "check_datasets([fed_weighted_dls[6].dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you're curious, here are all of the worker counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_datasets([dl.dataset for dl in fed_weighted_dls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the federated approach performs with variation in the worker datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_weighted_mgr = FederatedManager('Fed Bias', fed_weighted_dls, default_testing_dataset, DefaultNet)\n",
    "fed_weighted_mgr.learn(default_n_rounds, default_target_accuracy)\n",
    "plot_managers(fed_weighted_mgr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biased federated model converges just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_managers([nonfed_alldata_mgr, fed_equal_mgr, fed_weighted_mgr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it converges a little slower than the unbiased federated model.\n",
    "\n",
    "Let's take a closer look at how performance changes with increasingly biased data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from federated import evaluate_new_manager\n",
    "\n",
    "mgrs = [evaluate_new_manager('Fed Bias '+ str(bias), default_training_dataset, default_testing_dataset, p=bias, n_rounds=default_n_rounds, target_accuracy=default_target_accuracy, model=DefaultNet) for bias in tqdm([0.0, 0.4, 0.8, 0.9, 0.95, 0.98, 0.99, 1.0], desc='Managers')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_managers(mgrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The federated models converge even with very heavily biased datasets per worker. Unsurprisingly, it takes longer to hit a given target accuracy. But even when the datasets are completely biased, that is, each worker sees examples from *exactly one class*, a federated set of workers can still at least converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In cases of heavily skewed datasets, federation enables the common model to incorporate signal from sources that operate only as sensors for outlier data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "TODO: WRAP UP THE BLOG POST HERE. EVERYTHING BELOW IS PART TWO OR THREE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
