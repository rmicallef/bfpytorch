{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federation with Varying Example Counts\n",
    "\n",
    "Now we've seen federation work with a dataset scattered across a number of workers. And we saw that its performance isn't substantially different from the non-federated approach, while decentralizing the work of training and limiting the amount of data transfered between the main manager and the workers.\n",
    "\n",
    "But what if our workers don't have access to equal amounts of data? Let's explore that.\n",
    "\n",
    "## Spliting the Deck into Uneven Piles\n",
    "\n",
    "To test this, we need workers to have access to different numbers of training examples. So let's make a set of decks that gives each of our workers more or less training data than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "TODO: WRAP UP THE BLOG POST HERE. EVERYTHING BELOW IS PART TWO OR THREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Post Two\n",
    "\n",
    "In our last post [LINK HERE] we showed how to implement federated learning in pytorch. In this post we get one step more realistic about how federated learning would play out in real life.\n",
    "\n",
    "In federated learning, we expect that each of the workers captures and trains on somewhat different data. For example, my mobile phone will capture more songs in genres of music that I listen to, while your cell phone will reflect a different set of sings in different genres. Probably. If not, get out of my mind! [BENE GESSIRIT MOTHER IMAGE? SINGLE WHITE FEMALE IMAGE?] And there's likely a lot of overlap due to radios and clustering of popular songs.\n",
    "\n",
    "To reflect this kind of data distribution, we're going to skew the MNIST data to reflect that each worker sees a somewhat different subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking the Deck (Skewing Data)\n",
    "\n",
    "We know the baseline data is pretty even across numerals. Now we need a way to \"stack the deck\" of examples that each worker sees. This method creates a dataset that is randomly sampled from a given dataset with the random sampling biased according to a dictionary of weights for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def stacked_dset(dset, label_weights, N):\n",
    "    \"\"\"\n",
    "        dset: dataset\n",
    "        label_weights = {dog: 0.5, cat: 0.3, ...}\n",
    "        N: size of stacked dset\n",
    "        return: stacked WeightedRandomSampler\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for data, label in dset:\n",
    "        weights.append(label_weights[label])\n",
    "\n",
    "#     for label in test_dset.targets:\n",
    "#         weights.append(label_weights[int(label)])\n",
    "# TODO / MLW : how to speed this up - currently takes about a minute to train ten stacked training sets\n",
    "    \n",
    "    return WeightedRandomSampler(weights, N, replacement=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is where we get the dictionary of weights. For simplicity's sake, we just take a list of labels to be sampled \"normally\" and the rest are biased against. So, preserving `3`s and skewing everything else by a factor of 0.9 shoud get a set of weights that results in a dataset that is slightly heavy on `3`s compared to everything else. In an an extreme example, preserving only `3`s, with a skew of 0, will produce weights that will yield a dataset of only `3`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewed_weights(num_labels, labels_to_preserve, skew_bias):\n",
    "    \"\"\"\n",
    "        num_labels: number of labels to return (use 10 for MNIST)\n",
    "        labels_to_preserve: list of labels to preserve wih no skew \n",
    "        skew_bias: a float, 0 < bias < 1, to which non-selected labels will be biased down\n",
    "        return: dictionary of each label and its bias\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    for label in range(num_labels):\n",
    "        if label in labels_to_preserve:\n",
    "            weights[label] = 1\n",
    "        else:\n",
    "            weights[label] = skew_bias\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the sampling to create our skewed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a54f84aface>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create stacked loaders for the workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Skew Bias'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskew_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrun_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Examples Per Skewed Loader'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrun_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Number of Workers'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_data' is not defined"
     ]
    }
   ],
   "source": [
    "# create stacked loaders for the workers\n",
    "\n",
    "run_data['Skew Bias'] = skew_bias = 1\n",
    "run_data['Examples Per Skewed Loader'] = loader_size = 60000\n",
    "run_data['Number of Workers'] = num_workers = 1\n",
    "\n",
    "stacking_start_time = time.time()\n",
    "\n",
    "stacked_data_loaders = []\n",
    "for label in tqdm(range(num_workers)):\n",
    "    stacked_sampler = stacked_dset(train_dset, skewed_weights(10, [label%10], skew_bias), loader_size)\n",
    "    stacked_data_loaders.append(DataLoader(train_dset, batch_size=batch_size, shuffle=False, sampler=stacked_sampler))\n",
    "\n",
    "run_data['Stacking Time'] = time.time() - stacking_start_time\n",
    "run_data['Stacking Time per Loader'] = run_data['Stacking Time'] / run_data['Number of Workers']\n",
    "\n",
    "print('Stacking Time: %.2f' % run_data['Stacking Time'])\n",
    "print('Stacking Time per Loader: %.2f' % run_data['Stacking Time per Loader'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the effect of the skew in a count and histogram of a skewed dataset. Here, we arbitrarily picked the second dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5fd6c78a960c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mybatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstacked_data_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataloader sample count:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mybatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/venv/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/venv/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/venv/lib/python3.6/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_, ybatches = list(zip(*stacked_data_loaders[0]))\n",
    "print('Dataloader sample count:', len(torch.cat(ybatches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(stacked_data_loaders)):\n",
    "#    _, ybatches = list(zip(*stacked_data_loaders[i]))\n",
    "#    print('Dataloader', i ,'sample count:', len(torch.cat(ybatches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "hist_counts = []\n",
    "digit_counts = []\n",
    "for loader in tqdm(stacked_data_loaders):\n",
    "    _, ybatches = list(zip(*loader))\n",
    "    ys = torch.cat(ybatches)\n",
    "    ys = [int(y) for y in ys]\n",
    "    hist_counts.append(ys)\n",
    "    \n",
    "    digits = sorted(Counter(ys).most_common())\n",
    "    _, digits = list(zip(*digits))\n",
    "    digit_counts.append(list(digits))\n",
    "\n",
    "digit_counts = [list(i) for i in zip(*digit_counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "fig.suptitle('Digit Skew Histogram')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.set_ylabel('Digit Count')\n",
    "ax.set_xlabel('Digit')\n",
    "H = ax.hist(ys, bins=range(11), histtype='bar', align='left', rwidth=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "fig.suptitle('Skew: Digit Counts by Worker')\n",
    "\n",
    "pos = list(range(num_workers))\n",
    "width = 0.08\n",
    "\n",
    "for digit in range(10):\n",
    "    ax.bar([p + (width * digit) for p in pos],\n",
    "           digit_counts[digit],\n",
    "           width = width,\n",
    "           label = str(digit),\n",
    "          )\n",
    "\n",
    "ax.set_xticks([p + (4.5 * width) for p in pos])\n",
    "ax.set_xticklabels([('Dataset ' + str(x)) for x in range(num_workers)])\n",
    "ax.set_ylabel('Digit Samples')\n",
    "ax.set_xlabel('Samples Grouped by Worker')\n",
    "ax.legend(loc = 'upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the `federatedManager` using the skewed training data. Note that we don't skew the test data -- we want to see how everything performs on a normal data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import federated\n",
    "\n",
    "run_data['Learning Rate'] = learning_rate = 1e-2\n",
    "run_data['Epochs per Round'] = num_epochs = 1\n",
    "run_data['Federated Training Rounds'] = num_rounds = 50\n",
    "\n",
    "manager = federated.FederatedManager(\n",
    "    stacked_data_loaders,\n",
    "    MLPNet,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    learning_rate,\n",
    "    test_dset,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some rounds of federated training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\", num_rounds, \"round(s) with\", manager.n_workers, \"worker(s) doing\", num_epochs, \"epoch(s) per round.\\n\" )\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(num_rounds)):\n",
    "    print(\"Beginning round\", i+1)\n",
    "    manager.round()\n",
    "    print(\"Finished round\", i+1, \"with global loss: %.2f\" % manager.manager_loss_history[-1], \"\\n\")\n",
    "\n",
    "run_data['Federated Training Time'] = time.time() - training_start_time\n",
    "#run_data['Manager Loss History'] = manager.manager_loss_history\n",
    "#run_data['Worker Loss Histories'] = manager.worker_loss_histories\n",
    "run_data['Final Global Loss'] = manager.manager_loss_history[-1]\n",
    "\n",
    "print('Federated Training Time: %.2f' % run_data['Federated Training Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at how the training went. Here's a graph of the loss per round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "# loss of global model on test set gets recorded twice per round\n",
    "# [1::2] skips the record that takes place before that round's training has happened\n",
    "ax.plot(manager.manager_loss_history[1::2], label=\"Global Loss\", )\n",
    "ax.set_xlabel(\"Federated Round\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good, with loss dropping off just like we want. Perhaps it's a little bumpy because of the relatively fast training rate, but it should be improving on balance. But if we look under the hood at each individual worker's loss, we see that the workers' local models are diverging and converging at each round. They diverge because each local model trains on different data, resulting in a somewhat different loss per round. The converge again because the manager combines them into a master model, such that they all have the same loss as the global loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "\n",
    "for i in range(len(manager.worker_loss_histories)):\n",
    "    ax.plot(manager.worker_loss_histories[i], label=('Worker ' + str(i)))\n",
    "\n",
    "# TODO: Align the global loss properly\n",
    "ax.plot(manager.manager_loss_history[1], label=\"Global Loss\", )\n",
    "\n",
    "    \n",
    "# TODO: Get these labels done properly - they should be aligned with the main \n",
    "ax.set_xticklabels([(i-1) for i in range(len(manager.worker_loss_histories))])\n",
    "ax.set_xlabel(\"Federated Round\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas:\n",
    "- plot performance on a given numeral for the main model next to that of a worker skewed against that numeral. Let both run without federation or run a few epochs before federation. Show this as a baseline\n",
    "- histogram of numerals? More for curiosity, but shows spread of data that we might want to reflect in the baseline training.\n",
    "- post 1: what's the accuracy loss for federation compared to baseline direct training?\n",
    "- post 2: weird side stats\n",
    "    - skew vs. accuracy\n",
    "        - plot - x-axis = skew, y-axis = accuracy\n",
    "    - run all to convergence, compare how long to reach comparable accuracy?\n",
    "        - time or epochs necessary to reach comparable accuracy between federated and standard approach\n",
    "        - time or epochs necessary to reach comparable accuracy by skew\n",
    "    - run the federated version with balanced, but small sets of data\n",
    "\n",
    "Questions:\n",
    "- Why does the time spent by a worker on any given epoch all happen _before_ the batches start rolling in? What's happening there? Am I just spinning my wheels on something?\n",
    "    - TODO: try this from a regular python file. The notebook may be buffering up those print statements in the batches\n",
    "- Why does random selection of the skewed datasets take so long? Is it because they're without replacement?\n",
    "- Why do all the workers and epochs always happen in order? Wouldn't my laptop parallelize them across cores? Is that too much to ask from an interpreter? Is the interpreter smarter than I am and actually is parallelizing them and the smartest way in to do them in order?\n",
    "- why use ten workers? Why not fewer?\n",
    "\n",
    "- TODO: unequal data volume at each worker. Try some workers with very small or very large samples.\n",
    "- TODO: unequal numbers of samples across the whole set, e.g., we just have fewer `7`s and `4`s across the set, and a glut of `1`s\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvement Ideas\n",
    "\n",
    "Global variables:\n",
    "- Batch size\n",
    "- Learning rate\n",
    "- Epochs\n",
    "- Total dataset size\n",
    "- Worker dataset size\n",
    "    - worker dataset size skew (variance among number of samples seen from worker to worker)\n",
    "- Selection of data with or without replacement\n",
    "- Dataset class skew (more or fewer examples from each class)\n",
    "\n",
    "Targets:\n",
    "- Loss\n",
    "- Accuracy\n",
    "- Runtime to target loss or accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Write a bit of code that records the hyperparameters and saves the graphs, times and losses in a bundle for each run. Something like:\n",
    "\n",
    "```\n",
    "2019-05-06 21:02:50\n",
    "\n",
    "# standard dataloader parameter\n",
    "batch_size = 128\n",
    "\n",
    "# biasing parameters\n",
    "skew_bias = 0.3\n",
    "loader_size = 8192\n",
    "num_workers = 10\n",
    "\n",
    "Stacked set creation time: 00:01:08\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 1\n",
    "num_rounds = 20\n",
    "\n",
    "Train time = 00:43:02\n",
    "\n",
    "Final global loss: 0.48251\n",
    "```\n",
    "\n",
    "Well. I did this. And now the code is unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a little performance info on the run\n",
    "run_data['Global End Time'] = time.time()\n",
    "run_data['Global Time'] = run_data['Global End Time'] - run_data['Global Start Time']\n",
    "run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave a record of the run\n",
    "# but it isn't valid JSON\n",
    "import json \n",
    "with open('run_data.json', 'a') as file:\n",
    "    file.write(json.dumps(run_data))\n",
    "    file.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "train_counts = Counter(int(y) for y in train_dset.targets).most_common()\n",
    "print(\"Train digit counts: \\n\", train_counts)\n",
    "print(\"Train count standard deviation: %.2f\" % np.std(list(zip(*train_counts))[1]))\n",
    "print(\"Train count coefficient of variation: %.2f\" \n",
    "      % (float(np.mean(list(zip(*train_counts))[1])) / float(np.std(list(zip(*train_counts))[1]))))\n",
    "\n",
    "print()\n",
    "\n",
    "test_counts = Counter(int(y) for y in test_dset.targets).most_common()\n",
    "print(\"Test digit counts: \\n\", test_counts)\n",
    "print(\"Test standard deviation: %.2f\" % np.std(list(zip(*test_counts))[1]))\n",
    "print(\"Test count coefficient of variation: %.2f\" \n",
    "      % (float(np.mean(list(zip(*test_counts))[1])) / float(np.std(list(zip(*test_counts))[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "fig.suptitle('Digit Counts at each Worker')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.set_xticklabels([('Digit ' + str(x-1)) for x in range(11)])\n",
    "ax.hist(hist_counts, \n",
    "        label=[('Worker ' + str(x)) for x in range(num_workers)],\n",
    "        bins=list(range(12)), \n",
    "        histtype='bar',\n",
    "        align='left',\n",
    "        rwidth=0.8,\n",
    "       );\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
