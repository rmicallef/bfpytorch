{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Machine Learning with MNIST\n",
    "\n",
    "This notebook is a demonstration of _federated learning_, a type of machine learning in which a machine learning task is addressed using multiple devices. Let's first talk about what we're going to see here.\n",
    "\n",
    "## The Task: Digit Recognition\n",
    "\n",
    "For this demonstration, our task is to determine which number is written in an image of a handwritten digit like this one: \n",
    "\n",
    "![Handwritten digit \"6\"](images/example-6.png)\n",
    "\n",
    "To carry out this task we need a machine learning model to \"look\" at an image and classify it into a number, and image data to train the model. We're going to use a simple convolutional neural network model. For the training images, we're using the well-known MNIST digits dataset.\n",
    "\n",
    "## The Approach: Federation\n",
    "\n",
    "The federated approach uses several _worker_ devices to train their own local models with independent data. That is, the workers each train their local model with a unique set of data.\n",
    "\n",
    "The workers train their models for a bit, then each worker sends its local model back to a _manager_. The manager combines the local models from all of the workers into a master model, then sends the master model back to the workers. This process is called a _round_. Then another round begins: the workers train their copy of the master model (creating another set of individualized local models), the manager combines these individualized local models into a new master model, and pushes the new master model down to the workers. This cycle of rounds repeats as long as is necessary and useful.\n",
    "\n",
    "## Our Federation Library\n",
    "\n",
    "The `federated` library we're using here includes a `FederatedManager` class and a `FederatedWorker` class. A `FederatedManager` contains a master prediction model and creates several independent `FederatedWorker` instances. Each `FederatedWorker` has its own local model and a distinct subset of the MNIST data.\n",
    "\n",
    "In each `round` of federated learning, each `FederatedWorker` trains its local model using only the data that that worker recieves, and then sends its trained local model to the `FederatedManager`. The `FederatedManager` combines the models into an updated master model and pushes that new master model down to each `FederatedWorker`.\n",
    "\n",
    "We're going to see how federation works and test out some corner cases here. Let's get started with the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "\n",
    "We need to load up the common elements to be used in the traditional and federated machine learning approaches. But we do a little housekeeping first to track performance of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "run_data = {} # used for code profiling\n",
    "\n",
    "run_data['Date'] = datetime.datetime.today().strftime('%Y-%m-%d %H:%M:%S')\n",
    "run_data['Global Start Time'] = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Now we define the model. We're using a simple multilayer perceptron in the `torch` framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\"\n",
    "\n",
    "# # Optional model for fun\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class LeNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LeNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x, dim=0) # value 0 was chosen arbitrarily to quiet a warning. Penny'll start a fire.\n",
    "\n",
    "#     def name(self):\n",
    "#         return 'LeNet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data - Standard MNIST Datasets\n",
    "Here we get plain vanilla MNIST data, a training set and a test set. Nothing exciting to see here. This is our baseline. \n",
    "\n",
    "If you don't already have the MNIST data on your machine, the setting `download=True` in the call to `torchvision.datasets.MNIST()` will fetch the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# create standard datasets using all of the MNIST data\n",
    "\n",
    "data_path = './MNIST-data/raw'\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "train_dset = dsets.MNIST(root=data_path, download=True, train=True, transform=trans)\n",
    "test_dset = dsets.MNIST(root=data_path, download=True, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our standard MNIST datasets: `train_dset` contains 60,000 examples, and `test_dset` contains 10,000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: EXPLAIN WHY WE NORMALIZE THE DATA, AND WHY WE USE THOSE VALUES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "run_data['Batch Size'] = batch_size = 512\n",
    "\n",
    "# create standard dataloaders using all of the MNIST data - this is for baseline purposes\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the default data. The out-of-the-box MNIST dataset has roughly equal numbers of samples for each digit, i.e., about as many `4`s as `6`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "train_counts = Counter(int(y) for y in train_dset.targets).most_common()\n",
    "print(\"Train digit counts: \\n\", train_counts)\n",
    "print(\"Train count standard deviation: %.2f\" % np.std(list(zip(*train_counts))[1]))\n",
    "print(\"Train count coefficient of variation: %.2f\" \n",
    "      % (float(np.mean(list(zip(*train_counts))[1])) / float(np.std(list(zip(*train_counts))[1]))))\n",
    "\n",
    "print()\n",
    "\n",
    "test_counts = Counter(int(y) for y in test_dset.targets).most_common()\n",
    "print(\"Test digit counts: \\n\", test_counts)\n",
    "print(\"Test standard deviation: %.2f\" % np.std(list(zip(*test_counts))[1]))\n",
    "print(\"Test count coefficient of variation: %.2f\" \n",
    "      % (float(np.mean(list(zip(*test_counts))[1])) / float(np.std(list(zip(*test_counts))[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is reasonably well distributed, and the plots below confirm it. We scaled up the test set counts by a factor of six to make them comparable to the training set, which has exactly six times as many examples as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "fig.suptitle('Standard MNIST Digit Counts')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.hist([train_dloader.dataset.targets.tolist(), test_dloader.dataset.targets.tolist()*6], \n",
    "        label=['Train', 'Test (scaled)'],\n",
    "        bins=list(range(11)), \n",
    "        histtype='bar',\n",
    "        align='left',\n",
    "        rwidth=0.8,\n",
    "       )\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Training?\n",
    "\n",
    "NOTE: DO WE WANT TO JUST TRAIN THE BASELINE MODEL HERE BEFORE STACKING THE DECK?\n",
    "\n",
    "AND THEN DO WE WANT TO SHOW A MODEL TRAINED ONLY ON ONE SKEWED DATASET?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking the Deck (Skewing Data)\n",
    "\n",
    "We know the baseline data is pretty even across numerals. Now we need a way to \"stack the deck\" of examples that each worker sees. This method creates a dataset that is randomly sampled from a given dataset with the random sampling biased according to a dictionary of weights for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def stacked_dset(dset, label_weights, N):\n",
    "    \"\"\"\n",
    "        dset: dataset\n",
    "        label_weights = {dog: 0.5, cat: 0.3, ...}\n",
    "        N: size of stacked dset\n",
    "        return: stacked WeightedRandomSampler\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for data, label in dset:\n",
    "        weights.append(label_weights[label])\n",
    "\n",
    "#     for label in test_dset.targets:\n",
    "#         weights.append(label_weights[int(label)])\n",
    "# TODO / MLW : how to speed this up - currently takes about a minute to train ten stacked training sets\n",
    "    \n",
    "    return WeightedRandomSampler(weights, N, replacement=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is where we get the dictionary of weights. For simplicity's sake, we just take a list of labels to be sampled \"normally\" and the rest are biased against. So, preserving `3`s and skewing everything else by a factor of 0.9 shoud get a set of weights that results in a dataset that is slightly heavy on `3`s compared to everything else. In an an extreme example, preserving only `3`s, with a skew of 0, will produce weights that will yield a dataset of only `3`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewed_weights(num_labels, labels_to_preserve, skew_bias):\n",
    "    \"\"\"\n",
    "        num_labels: number of labels to return (use 10 for MNIST)\n",
    "        labels_to_preserve: list of labels to preserve wih no skew \n",
    "        skew_bias: a float, 0 < bias < 1, to which non-selected labels will be biased down\n",
    "        return: dictionary of each label and its bias\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    for label in range(num_labels):\n",
    "        if label in labels_to_preserve:\n",
    "            weights[label] = 1\n",
    "        else:\n",
    "            weights[label] = skew_bias\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the sampling to create our skewed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# create stacked loaders for the workers\n",
    "\n",
    "run_data['Skew Bias'] = skew_bias = 0.2\n",
    "run_data['Examples Per Skewed Loader'] = loader_size = 512\n",
    "run_data['Number of Workers'] = num_workers = 10\n",
    "\n",
    "stacking_start_time = time.time()\n",
    "\n",
    "stacked_data_loaders = []\n",
    "for label in tqdm(range(num_workers)):\n",
    "    stacked_sampler = stacked_dset(train_dset, skewed_weights(10, [label%10], skew_bias), loader_size)\n",
    "    stacked_data_loaders.append(DataLoader(train_dset, batch_size=batch_size, shuffle=False, sampler=stacked_sampler))\n",
    "\n",
    "run_data['Stacking Time'] = time.time() - stacking_start_time\n",
    "run_data['Stacking Time per Loader'] = run_data['Stacking Time'] / run_data['Number of Workers']\n",
    "\n",
    "print('Stacking Time: %.2f' % run_data['Stacking Time'])\n",
    "print('Stacking Time per Loader: %.2f' % run_data['Stacking Time per Loader'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the effect of the skew in a count and histogram of a skewed dataset. Here, we arbitrarily picked the second dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ybatches = list(zip(*stacked_data_loaders[1]))\n",
    "ys = torch.cat(ybatches)\n",
    "ys = [int(y) for y in ys]\n",
    "print (Counter(ys).most_common())\n",
    "\n",
    "# counts = sorted(Counter(ys).most_common())\n",
    "# print(counts)\n",
    "# _, counts = zip(*counts)\n",
    "# print(list(counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "fig.suptitle('Skew Example')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "H = ax.hist(ys, bins=range(11), histtype='bar', align='left', rwidth=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_counts = []\n",
    "digit_counts = []\n",
    "for loader in tqdm(stacked_data_loaders):\n",
    "    _, ybatches = list(zip(*loader))\n",
    "    ys = torch.cat(ybatches)\n",
    "    ys = [int(y) for y in ys]\n",
    "    hist_counts.append(ys)\n",
    "    \n",
    "    digits = sorted(Counter(ys).most_common())\n",
    "    _, digits = list(zip(*digits))\n",
    "    digit_counts.append(list(digits))\n",
    "\n",
    "digit_counts = [list(i) for i in zip(*digit_counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "fig.suptitle('Digit Counts at each Worker')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.set_xticklabels([('Digit ' + str(x-1)) for x in range(11)])\n",
    "ax.hist(hist_counts, \n",
    "        label=[('Worker ' + str(x)) for x in range(num_workers)],\n",
    "        bins=list(range(12)), \n",
    "        histtype='bar',\n",
    "        align='left',\n",
    "        rwidth=0.8,\n",
    "       );\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "# fig.suptitle('Skew Examples')\n",
    "# ax.xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "# ax.hist(digit_counts, \n",
    "#         label=[('Digit ' + str(x)) for x in range(11)],\n",
    "#         bins=num_workers + 1, \n",
    "#         histtype='bar',\n",
    "#         align='left',\n",
    "#         rwidth=0.8,\n",
    "#        );\n",
    "# ax.legend();\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "fig.suptitle('Skew Example: Digit Counts by Worker')\n",
    "\n",
    "pos = list(range(num_workers))\n",
    "width = 0.08\n",
    "\n",
    "for digit in range(10):\n",
    "    ax.bar([p + (width * digit) for p in pos],\n",
    "           digit_counts[digit],\n",
    "           width = width,\n",
    "           label = 'Digit ' + str(digit),\n",
    "          )\n",
    "\n",
    "ax.set_xticks([p + (4.5 * width) for p in pos])\n",
    "ax.set_xticklabels([('Wrkr' + str(x)) for x in range(num_workers)])\n",
    "ax.set_ylabel('Digit Samples')\n",
    "ax.legend(loc = 'upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the `federatedManager` using the skewed training data. Note that we don't skew the test data -- we want to see how everything performs on a normal data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import federated\n",
    "\n",
    "run_data['Learning Rate'] = learning_rate = 5e-2\n",
    "run_data['Epochs per Round'] = num_epochs = 3\n",
    "run_data['Federated Training Rounds'] = num_rounds = 6\n",
    "\n",
    "manager = federated.FederatedManager(\n",
    "    stacked_data_loaders,\n",
    "    MLPNet,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    learning_rate,\n",
    "    test_dset,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\", num_rounds, \"round(s) with\", manager.n_workers, \"worker(s) doing\", num_epochs, \"epoch(s) per round.\\n\" )\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(num_rounds)):\n",
    "    print(\"Beginning round\", i+1)\n",
    "    manager.round()\n",
    "    print(\"Finished round\", i+1, \"with global loss: %.2f\" % manager.manager_loss_history[-1], \"\\n\")\n",
    "\n",
    "run_data['Federated Training Time'] = time.time() - training_start_time\n",
    "#run_data['Manager Loss History'] = manager.manager_loss_history\n",
    "#run_data['Worker Loss Histories'] = manager.worker_loss_histories\n",
    "run_data['Final Global Loss'] = manager.manager_loss_history[-1]\n",
    "\n",
    "print('Federated Training Time: %.2f' % run_data['Federated Training Time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at how the training went. Here's a graph of the loss per round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "# loss of global model on test set gets recorded twice per round\n",
    "# [1::2] skips the record that takes place before that round's training has happened\n",
    "ax.plot(manager.manager_loss_history[1::2], label=\"Global Loss\", )\n",
    "ax.set_xlabel(\"Federated Round\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good, with loss dropping off just like we want. Perhaps it's a little bumpy because of the relatively fast training rate, but it should be improving on balance. But if we look under the hood at each individual worker's loss, we see that the workers' local models are diverging and converging at each round. They diverge because each local model trains on different data, resulting in a somewhat different loss per round. The converge again because the manager combines them into a master model, such that they all have the same loss as the global loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "\n",
    "for i in range(len(manager.worker_loss_histories)):\n",
    "    ax.plot(manager.worker_loss_histories[i], label=('Worker ' + str(i)))\n",
    "\n",
    "# TODO: Align the global loss properly\n",
    "ax.plot(manager.manager_loss_history[1], label=\"Global Loss\", )\n",
    "\n",
    "    \n",
    "# TODO: Get these labels done properly - they should be aligned with the main \n",
    "ax.set_xticklabels([(i-1) for i in range(len(manager.worker_loss_histories))])\n",
    "ax.set_xlabel(\"Federated Round\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Federated Approach\n",
    "\n",
    "To simulate non-federated learning, we train a model using just one worker. This model will have train on the standard MNIST dataset instead of the skewed subsets of MNIST we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data['Learning Rate'] = learning_rate = 5e-2\n",
    "run_data['Epochs per Round'] = num_epochs = 3\n",
    "run_data['Federated Training Rounds'] = num_rounds = 6\n",
    "\n",
    "default_manager = federated.FederatedManager(\n",
    "    [train_dloader],\n",
    "    MLPNet,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    learning_rate,\n",
    "    test_dset,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\", num_rounds, \"round(s) with\", default_manager.n_workers, \"worker(s) doing\", num_epochs, \"epoch(s) per round.\\n\" )\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(num_rounds)):\n",
    "    print(\"Beginning round\", i+1)\n",
    "    default_manager.round()\n",
    "    print(\"Finished round\", i+1, \"with global loss: %.5f\" % default_manager.manager_loss_history[-1], \"\\n\")\n",
    "\n",
    "run_data['Federated Training Time'] = time.time() - training_start_time\n",
    "#run_data['Manager Loss History'] = default_manager.manager_loss_history\n",
    "#run_data['Worker Loss Histories'] = default_manager.worker_loss_histories\n",
    "run_data['Final Global Loss'] = default_manager.manager_loss_history[-1]\n",
    "\n",
    "print('Federated Training Time: %.2f' % run_data['Federated Training Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig_default, ax_default = plt.subplots(figsize=(16, 9))\n",
    "# loss of global model on test set gets recorded twice per round\n",
    "# [1::2] skips the record that takes place before that round's training has happened\n",
    "ax_default.plot(default_manager.manager_loss_history[1::2], label=\"Global Loss\", )\n",
    "ax_default.set_xlabel(\"Federated Round\")\n",
    "ax_default.set_ylabel(\"Loss\")\n",
    "ax_default.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Write a bit of code that records the hyperparameters and saves the graphs, times and losses in a bundle for each run. Something like:\n",
    "\n",
    "```\n",
    "2019-05-06 21:02:50\n",
    "\n",
    "# standard dataloader parameter\n",
    "batch_size = 128\n",
    "\n",
    "# biasing parameters\n",
    "skew_bias = 0.3\n",
    "loader_size = 8192\n",
    "num_workers = 10\n",
    "\n",
    "Stacked set creation time: 00:01:08\n",
    "\n",
    "# training parameters\n",
    "learning_rate = 1e-2\n",
    "num_epochs = 1\n",
    "num_rounds = 20\n",
    "\n",
    "Train time = 00:43:02\n",
    "\n",
    "Final global loss: 0.48251\n",
    "```\n",
    "\n",
    "Well. I did this. And now the code is unreadable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- Set up a model and data\n",
    "- train the model N epochs without federation note baseline performance (and size of the data that would have to have been transferedd?) \n",
    "    - is this model trained on the full dataset? Or do we sample randomly across it to have the same number of examples?\n",
    "- Federate without skew or mild skew, compare performance with baseline (and size of model compared to data)\n",
    "- Federate with only a few numbers skewed (like, lacking only 7s or something) \n",
    "- Federate with heavy skew\n",
    "- Federate with complete skew\n",
    "\n",
    "Ideas:\n",
    "- plot performance on a given numeral for the main model next to that of a worker skewed against that numeral. Let both run without federation or run a few epochs before federation. Show this as a baseline\n",
    "- histogram of numerals? More for curiosity, but shows spread of data that we might want to reflect in the baseline training.\n",
    "- post 1: what's the accuracy loss for federation compared to baseline direct training?\n",
    "- post 2: weird side stats\n",
    "    - skew vs. accuracy\n",
    "        - plot - x-axis = skew, y-axis = accuracy\n",
    "    - run all to convergence, compare how long to reach comparable accuracy?\n",
    "        - time or epochs necessary to reach comparable accuracy between federated and standard approach\n",
    "        - time or epochs necessary to reach comparable accuracy by skew\n",
    "    - \n",
    "\n",
    "Questions:\n",
    "- Why does the time spent by a worker on any given epoch all happen _before_ the batches start rolling in? What's happening there? Am I just spinning my wheels on something?\n",
    "    - TODO: try this from a regular python file. The notebook may be buffering up those print statements in the batches\n",
    "- Why does random selection of the skewed datasets take so long? Is it because they're without replacement?\n",
    "- Why do all the workers and epochs always happen in order? Wouldn't my laptop parallelize them across cores? Is that too much to ask from an interpreter? Is the interpreter smarter than I am and actually is parallelizing them and the smartest way in to do them in order?\n",
    "- why use ten workers? Why not fewer?\n",
    "\n",
    "- TODO: unequal data volume at each worker. Try some workers with very small or very large samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a little performance info on the run\n",
    "run_data['Global End Time'] = time.time()\n",
    "run_data['Global Time'] = run_data['Global End Time'] - run_data['Global Start Time']\n",
    "run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave a record of the run\n",
    "# but it isn't valid JSON\n",
    "import json \n",
    "with open('run_data.json', 'a') as file:\n",
    "    file.write(json.dumps(run_data))\n",
    "    file.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
