{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Test\n",
    "\n",
    "This notebook is a test of federated learning using the MNIST dataset. It distributes partial subsets of the MNIST data to each worker and tests the results of federation of the workers. It also skews the subsets to investigate the value of federation in cases where workers have substantially different samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Baseline\n",
    "\n",
    "First, we load up the common elements to be used in the traditional and federated approaches. We start with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\"\n",
    "\n",
    "# # Optional model for fun\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class LeNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(LeNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.fc1 = nn.Linear(320, 50)\n",
    "#         self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.fc2(x)\n",
    "#         return F.log_softmax(x, dim=0) # value 0 was chosen arbitrarily to quiet a warning. Penny'll start a fire.\n",
    "\n",
    "#     def name(self):\n",
    "#         return 'LeNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# create standard datasets using all of the MNIST data\n",
    "\n",
    "data_path = './MNIST-data/raw'\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "train_dset = dsets.MNIST(root=data_path, download=True, train=True, transform=trans)\n",
    "test_dset = dsets.MNIST(root=data_path, download=True, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking a Deck\n",
    "\n",
    "We need a way to \"stack the deck\" of examples that each worker sees. This method creates a dataset that is randomly sampled from a given dataset with the random sampling biased according to a dictionary of weights for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def stacked_dset(dset, label_weights, N):\n",
    "    \"\"\"\n",
    "    dset: dataset\n",
    "    label_weights = {dog: 0.5, cat: 0.3, ...}\n",
    "    N: size of stacked dset\n",
    "    return: stacked WeightedRandomSampler\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for data, label in dset:\n",
    "        weights.append(label_weights[label])\n",
    "    return WeightedRandomSampler(weights, N, replacement=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is where we get the dictionary of weights. For simplicity's sake, we just take a list of labels to be sampled \"normally\" and the rest are biased against. So, preserving 3s and skewing everything else by a factor of 0.9 shoud get a set of weights that results in a dataset that is slightly heavy on 3s compared to everything else. In an an extreme example, preserving only 3s, with a skew of 0, will produce weights that will yield a dataset of only 3s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewed_weights(num_labels, labels_to_preserve, skew_bias):\n",
    "    \"\"\"\n",
    "        num_labels: number of labels to return (use 10 for MNIST)\n",
    "        labels_to_preserve: list of labels to preserve wih no skew \n",
    "        skew_bias: a float, 0 < bias < 1, to which non-selected labels will be biased down\n",
    "        return: dictionary of each label and its bias\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    for label in range(num_labels):\n",
    "        if label in labels_to_preserve:\n",
    "            weights[label] = 1\n",
    "        else:\n",
    "            weights[label] = skew_bias\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# # create standard dataloaders using all of the MNIST data - this is for baseline purposes\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [(1, 1135), (2, 1032), (7, 1028), (3, 1010), (9, 1009), (4, 982), (0, 980), (8, 974), (6, 958), (5, 892)]\n",
      "Train: [(1, 6742), (7, 6265), (3, 6131), (2, 5958), (9, 5949), (0, 5923), (6, 5918), (8, 5851), (4, 5842), (5, 5421)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "_, ys = list(zip(*test_dloader))\n",
    "print(\"Test: \", Counter(int(y) for y in torch.cat(ys)).most_common())\n",
    "\n",
    "_, ys = list(zip(*train_dloader))\n",
    "print(\"Train:\", Counter(int(y) for y in torch.cat(ys)).most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots show quickly that the sets are distributed fairly similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: you don't need plt.show in notebooks, that's a python script thing.\n",
    "\n",
    "Note the semi-colon on the last line. That's a notebook trick to suppress printing the return value of the final statemnet (i.e. that weird `Patch` object that `ax.hist(...)` returns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_title('Testing examples')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.hist(test_dloader.dataset.targets, bins=range(11), histtype='bar', align='left', rwidth=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_title('Training examples')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.hist(train_dloader.dataset.targets, bins=range(11), histtype='bar', align='left', rwidth=0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do the sampling to create our skewed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:18<00:00,  9.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# create stacked loaders for the workers\n",
    "\n",
    "skew_bias = 0.5\n",
    "loader_size = 2048\n",
    "num_workers = 2\n",
    "\n",
    "stacked_data_loaders = []\n",
    "\n",
    "for label in tqdm(range(num_workers)):\n",
    "    stacked_sampler = stacked_dset(train_dset, skewed_weights(10, [label], skew_bias), loader_size)\n",
    "    stacked_data_loaders.append(DataLoader(train_dset, batch_size=batch_size, shuffle=False, sampler=stacked_sampler))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the effect of the skew in a histogram of a skewed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here's the problem\n",
    "\n",
    "The stacked_dataloader.target attributes, which should in principle just be all the labels in the dataset is for some reason not picking up on the stacking. It's just a copy of the `target` attribute of the original dataset. IMO that's a bug in pytorch. In any case, the consequence is, if you need all the labels in a dataset in one object, you can't just access the target attribute, you need to iterate through the list.\n",
    "\n",
    "I don't _think_ the same bug exists in federated.py. It doesn't access `target` directly. It iterates (i.e. sees the stacked data). Although it might be judicious to check this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is correct\n",
    "\n",
    "This is the code you had before, just broken out to be clearer for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 415),\n",
       " (4, 206),\n",
       " (8, 198),\n",
       " (7, 198),\n",
       " (3, 194),\n",
       " (0, 184),\n",
       " (6, 170),\n",
       " (9, 169),\n",
       " (5, 169),\n",
       " (2, 145)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, ybatches = list(zip(*stacked_data_loaders[1]))\n",
    "\n",
    "# ybatches are like tensor([1,2,3,5,0,5], [1,2,3,0,1,8], ...), i.e. lots of batches.\n",
    "# we need to join them together:\n",
    "\n",
    "ys = torch.cat(ybatches)\n",
    "\n",
    "# now we have tensor([1,2,3,5,0,5,1,2,3,0,1,8,...)\n",
    "\n",
    "# and then we need to deal with the fact that Counter doesn't understand tensors\n",
    "# i.e. turn ys into a list of integers, rather than a long tensor\n",
    "\n",
    "ys = [int(y) for y in ys]\n",
    "\n",
    "Counter(ys).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 6742),\n",
       " (7, 6265),\n",
       " (3, 6131),\n",
       " (2, 5958),\n",
       " (9, 5949),\n",
       " (0, 5923),\n",
       " (6, 5918),\n",
       " (8, 5851),\n",
       " (4, 5842),\n",
       " (5, 5421)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(int(y) for y in stacked_data_loaders[1].dataset.targets).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So to make the plot\n",
    "\n",
    "We made `ys` a couple of cells ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_title('Skew examples')\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "ax.hist(ys, bins=range(11), histtype='bar', align='left', rwidth=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import federated\n",
    "\n",
    "learning_rate = 5e-3\n",
    "num_epochs = 1\n",
    "num_rounds = 5\n",
    "\n",
    "skewed_train_dsets = stacked_data_loaders\n",
    "\n",
    "manager = federated.FederatedManager(\n",
    "    skewed_train_dsets,\n",
    "    MLPNet,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    learning_rate,\n",
    "    test_dset,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5 round(s) with 10 worker(s) doing 1 epoch(s) each\n",
      "\n",
      "Beginning round 1\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2971\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3001\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2853\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2821\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2649 \n",
      "\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3065\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2967\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2934\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2960\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2950 \n",
      "\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3282\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3048\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.3048\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.3191\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.3094 \n",
      "\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3004\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2906\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2977\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2797\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2870 \n",
      "\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3065\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3056\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2803\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2903\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2982 \n",
      "\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3137\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3054\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.3027\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2976\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.3060 \n",
      "\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2989\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3116\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2913\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2996\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2871 \n",
      "\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2994\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2853\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2842\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2765\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2873 \n",
      "\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3123\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3133\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2926\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2831\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2901 \n",
      "\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3008\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3014\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.3020\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.3042\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.3008 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:38<02:33, 38.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished round 1 with global loss: 2.29564 \n",
      "\n",
      "Beginning round 2\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3064\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2888\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2759\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2592\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2626 \n",
      "\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3090\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2947\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2924\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2649\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2772 \n",
      "\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3036\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3049\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2953\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2856\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2879 \n",
      "\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2979\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2756\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2814\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2598\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2719 \n",
      "\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2793\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2818\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2848\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2804\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2871 \n",
      "\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3032\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2844\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2842\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2784\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2877 \n",
      "\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2966\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2679\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2879\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2746\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2750 \n",
      "\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2953\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2602\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2759\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2714\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2942 \n",
      "\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2976\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2827\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2897\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2712\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2669 \n",
      "\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.3182\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.3145\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2838\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2888\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2759 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [01:13<01:52, 37.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished round 2 with global loss: 2.28360 \n",
      "\n",
      "Beginning round 3\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2721\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2740\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2796\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2585\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2651 \n",
      "\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2807\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2738\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2648\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2423\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2591 \n",
      "\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2894\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2948\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2886\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2847\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2782 \n",
      "\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2657\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2685\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2686\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2616\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2513 \n",
      "\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2780\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2662\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2664\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2666\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2525 \n",
      "\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2806\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2816\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2929\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2706\n",
      "\tWorker: 6752 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2842 \n",
      "\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2750\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2617\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2697\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2620\n",
      "\tWorker: 6360 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2517 \n",
      "\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2830\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2696\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2582\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2632\n",
      "\tWorker: 7584 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2806 \n",
      "\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2687\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2851\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2717\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2808\n",
      "\tWorker: 4296 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2697 \n",
      "\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2890\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2851\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2789\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2713\n",
      "\tWorker: 4352 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2679 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [01:52<01:15, 37.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished round 3 with global loss: 2.27214 \n",
      "\n",
      "Beginning round 4\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2668\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2834\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2761\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2659\n",
      "\tWorker: 5904 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2676 \n",
      "\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2748\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2474\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2671\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2620\n",
      "\tWorker: 7768 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2308 \n",
      "\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2801\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2709\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2663\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2667\n",
      "\tWorker: 3368 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2564 \n",
      "\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2549\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2532\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2516\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 24 \tlocal loss: 2.2609\n",
      "\tWorker: 1856 \tepoch: 1 \tbatch: 32 \tlocal loss: 2.2406 \n",
      "\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 0 \tlocal loss: 2.2641\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 8 \tlocal loss: 2.2739\n",
      "\tWorker: 5984 \tepoch: 1 \tbatch: 16 \tlocal loss: 2.2583\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a36cd70f84a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beginning round\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished round\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"with global loss: %.5f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanager_loss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/federated.py\u001b[0m in \u001b[0;36mround\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mweighted\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreceive\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mserver\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfedavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/federated.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mweighted\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreceive\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mserver\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfedavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/federated.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m                 \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mypred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-236b4c674d57>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/venv/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/bfpytorch/venv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training\", num_rounds, \"round(s) with\", manager.n_workers, \"worker(s) doing\", num_epochs, \"epoch(s) each\\n\" )\n",
    "\n",
    "for i in tqdm(range(num_rounds)):\n",
    "    print(\"Beginning round\", i+1)\n",
    "    manager.round()\n",
    "    print(\"Finished round\", i+1, \"with global loss: %.5f\" % manager.manager_loss_history[-1], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# loss of global model on test set gets recorded twice per round\n",
    "# [1::2] skips the record that takes place before that round's training has happened\n",
    "ax.plot(manager.manager_loss_history[1::2], label=\"Global Loss\")\n",
    "ax.set_xlabel(\"Round\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(len(manager.worker_loss_histories)):\n",
    "    lbl = \"Worker \" + str(i)\n",
    "    ax.plot(manager.worker_loss_histories[i], label=lbl)\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline:\n",
    "- Set up a model and data\n",
    "- train the model N epochs without federation note baseline performance (and size of the data that would have to have been transferedd?) \n",
    "    - is this model trained on the full dataset? Or do we sample randomly across it to have the same number of examples?\n",
    "- Federate without skew or mild skew, compare performance with baseline (and size of model compared to data)\n",
    "- Federate with only a few numbers skewed (like, lacking only 7s or something) \n",
    "- Federate with heavy skew\n",
    "- Federate with complete skew\n",
    "\n",
    "Ideas:\n",
    "- plot performance on a given numeral for the main model next to that of a worker skewed against that numeral. Let both run without federation or run a few epochs before federation. Show this as a baseline\n",
    "- histogram of numerals? More for curiosity, but shows spread of data that we might want to reflect in the baseline training.\n",
    "- why use ten workers? Why not fewer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
