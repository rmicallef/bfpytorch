{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Test\n",
    "\n",
    "This notebook is a test of federated learning using the MNIST dataset. It distributes partial subsets of the MNIST data to each worker and tests the results of federation of the workers. It also skews the subsets to investigate the value of federation in cases where workers have substantially different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put this in this file to have imported models reload automatically when you edit them.\n",
    "#!cat ~/.ipython/profile_default/startup/00-autoreload.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "        self.fc2 = nn.Linear(500, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def name(self):\n",
    "        return \"MLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = './MNIST-data/raw'\n",
    "\n",
    "# location of data and labels\n",
    "test_labels_file = data_path + '/' + 't10k-labels-idx1-ubyte'\n",
    "test_data_file = data_path + '/' + 't10k-images-idx3-ubyte'\n",
    "train_labels_file = data_path + '/' + 'train-labels-idx1-ubyte'\n",
    "train_data_file = data_path + '/' + 'train-images-idx3-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create standard datasets using all of the MNIST data\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "train_dset = dsets.MNIST(root=data_path, download=True, train=True, transform=trans)\n",
    "test_dset = dsets.MNIST(root=data_path, download=True, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_dset(dset, label_weights, N):\n",
    "    \"\"\"\n",
    "    dset: dataset\n",
    "    label_weights = {dog: 0.5, cat: 0.3, ...}\n",
    "    N: size of stacked dset\n",
    "    return: stacked WeightedRandomSampler\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    for data, label in dset:\n",
    "        weights.append(label_weights[label])\n",
    "    return WeightedRandomSampler(weights, N, replacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewed_weights(num_labels, labels_to_preserve, skew_bias):\n",
    "    \"\"\"\n",
    "        num_labels: number of labels to return (use 10 for MNIST)\n",
    "        labels_to_preserve: list of labels to preserve wih no skew \n",
    "        skew_bias: a float, 0 < bias < 1, to which non-selected labels will be biased down\n",
    "        return: dictionary of each label and its bias\n",
    "    \"\"\"\n",
    "    weights = {}\n",
    "    for label in range(num_labels):\n",
    "        if label in labels_to_preserve:\n",
    "            weights[label] = 1\n",
    "        else:\n",
    "            weights[label] = skew_bias\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create standard dataloaders using all of the MNIST data\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dloader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_dloader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_size = 1024\n",
    "\n",
    "stacked_data_loaders = []\n",
    "for label in tqdm(range(10)):\n",
    "    #print(label)\n",
    "    stacked_sampler = stacked_dset(train_dset, skewed_weights(10, [label], 0.4), loader_size)\n",
    "    stacked_data_loaders.append(DataLoader(train_dset, batch_size=batch_size, shuffle=False, sampler=stacked_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "num_epochs = 1\n",
    "num_rounds = 3\n",
    "\n",
    "#datasets = [train_dloader,\n",
    "#            train_dloader,\n",
    "#            train_dloader,            \n",
    "#            ]\n",
    "\n",
    "datasets = stacked_data_loaders\n",
    "\n",
    "manager = federated.FederatedManager(\n",
    "    datasets,\n",
    "    MLPNet,\n",
    "    nn.CrossEntropyLoss(),\n",
    "    learning_rate,\n",
    "    test_dset,\n",
    "    num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\", num_rounds, \"round(s) with\", manager.n_workers, \"worker(s) doing\", num_epochs, \"epoch(s) each\" )\n",
    "\n",
    "for i in tqdm(range(num_rounds)):\n",
    "    print(\"Beginning round\", i)\n",
    "    manager.round()\n",
    "    print(\"    Round: %03d\" % i, \"Loss: %.5f\" % manager.manager_loss_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# loss of global model on test set gets recorded twice per round\n",
    "# [1::2] skips the record that takes place before that round's training has happened\n",
    "ax.plot(manager.manager_loss_history[1::2], label=\"Global Loss\")\n",
    "ax.set_xlabel(\"Round\");\n",
    "ax.legend();\n",
    "\n",
    "# loss of fig, ax = plt.subplots(5, 2)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Round\")\n",
    "for i in range(len(manager.worker_loss_histories)):\n",
    "    \n",
    "    print(manager.worker_loss_histories[i])\n",
    "    ax.plot(manager.worker_loss_histories[i][1::2], label=i)\n",
    "    lbl = \"Worker \" + str(i)\n",
    "\n",
    "ax.legend();\n",
    "\n",
    "## MLW: WHY DOES THE LAST WORKER SHOW UP FIRST IN THE LEGEND?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO - need to get the stacked decks combined to train a common model with all of them. It's not a fair comparison to have the non-federated approach exposed to the entire MNIST set, while the cumulative federated workers get less.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
